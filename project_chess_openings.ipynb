{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 8018\n",
    "drop_duplicates_for_projection = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network\n",
    "import tensorflow as tf\n",
    "dims = (8, 8, 13)\n",
    "n_components = 2\n",
    "encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=dims),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=16, kernel_size=3, strides=(2, 2), activation=\"relu\", padding=\"same\"\n",
    "    ),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=32, kernel_size=3, strides=(2, 2), activation=\"relu\", padding=\"same\"\n",
    "    ),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=64, kernel_size=3, strides=(2, 2), activation=\"relu\", padding=\"same\"\n",
    "    ),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=128, kernel_size=3, strides=(2, 2), activation=\"relu\", padding=\"same\"\n",
    "    ),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=256, kernel_size=3, strides=(2, 2), activation=\"relu\", padding=\"same\"\n",
    "    ),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=100),\n",
    "    tf.keras.layers.Dense(units=100),\n",
    "    tf.keras.layers.Dense(units=100),\n",
    "    tf.keras.layers.Dense(units=n_components),\n",
    "])\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_fit_kwargs = {\"callbacks\": [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='loss',\n",
    "        min_delta=10**-2,\n",
    "        patience=10,\n",
    "        verbose=1,\n",
    "    )\n",
    "]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('all_openings.csv')\n",
    "\n",
    "# One-hot encode the chessboard columns\n",
    "expected_categories = ['wp', 'wr', 'wn', 'wb', 'wq', 'wk', 'bp', 'br', 'bn', 'bb', 'bq', 'bk', '']\n",
    "\n",
    "# Define chessboard columns\n",
    "chessboard_columns = [f\"{col}{row}\" for row in range(1, 9) for col in \"abcdefgh\"]\n",
    "\n",
    "for column in chessboard_columns:\n",
    "    df[column] = pd.Categorical(df[column], categories=expected_categories)\n",
    "\n",
    "\n",
    "training_data_df = df[chessboard_columns]\n",
    "\n",
    "training_data_df = pd.get_dummies(training_data_df, columns=chessboard_columns)\n",
    "\n",
    "# Combine the one-hot encoded chessboard DataFrame with the rest of the metadata\n",
    "# First, drop the original chessboard columns from the main DataFrame to avoid duplicates\n",
    "metadata_df = df.drop(columns=chessboard_columns)\n",
    "# Then, concatenate the encoded chessboard DataFrame with the metadata DataFrame\n",
    "combined_df = pd.concat([metadata_df, training_data_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if drop_duplicates_for_projection:\n",
    "    # drop duplicates for projection, use the column names from chessboard_dummies_df for finding duplicates\n",
    "    print(\"Length before dropping duplicates:\", len(training_data_df))\n",
    "    training_data_df.drop_duplicates(subset=training_data_df.columns, inplace=True)\n",
    "    print(\"Length after dropping duplicates:\", len(training_data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the data correctly for the encoder\n",
    "# get numpy data out of training_data_df\n",
    "train_data = training_data_df.to_numpy()\n",
    "\n",
    "# Verify the shape\n",
    "print(\"Train shape:\", train_data.shape)\n",
    "\n",
    "# Generate embeddings using the encoder just to verify the shape\n",
    "train_embeddings = encoder.predict(train_data.reshape(-1, 8, 8, 13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap.parametric_umap import ParametricUMAP\n",
    "\n",
    "reducer = ParametricUMAP(\n",
    "    verbose=True,\n",
    "    keras_fit_kwargs = keras_fit_kwargs,\n",
    "    encoder=encoder,\n",
    "    dims=dims,\n",
    "    random_state=seed,\n",
    "    n_training_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorama\n",
    "reducer = ParametricUMAP(encoder=encoder, dims=dims)\n",
    "print(\"Before fitting, check dims:\", dims)\n",
    "print(\"Reducer expected input shape:\", reducer.dims)\n",
    "# Attempt to fit and transform\n",
    "try:\n",
    "    embedding = reducer.fit_transform(train_data.reshape((train_data.shape[0], -1)))\n",
    "    print(f\"Embedding shape after fit_transform: {embedding.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"{colorama.Fore.RED}Error during fit_transform: {e}{colorama.Style.RESET_ALL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from umap.parametric_umap import load_ParametricUMAP\n",
    "# embedder = load_ParametricUMAP('/your/path/here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unique filename based on date and time\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "reducer.save('./embeddings/parametric_umap_embeddings_' + now.strftime(\"%Y-%m-%d_%H-%M-%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# list of chess board dummy columns (to be used for hashing) = training_data_df columns\n",
    "chessboard_dummy_columns = training_data_df.columns\n",
    "\n",
    "# Add Projection Coordinates to Training Data DataFrame\n",
    "training_data_df[\"x\"] = embedding[:, 0]\n",
    "training_data_df[\"y\"] = embedding[:, 1]\n",
    "\n",
    "# Generate a unique hash for each row's configuration in both DataFrames\n",
    "def generate_row_hash(df, prefix_columns):\n",
    "    return df[prefix_columns].apply(lambda x: hash(tuple(x)), axis=1)\n",
    "\n",
    "# Generate hashes for both DataFrames\n",
    "training_data_hash = generate_row_hash(training_data_df, chessboard_dummy_columns)\n",
    "combined_data_hash = generate_row_hash(combined_df, chessboard_dummy_columns)  # Only the one-hot encoded columns\n",
    "\n",
    "# Add these hashes as a column to both DataFrames\n",
    "training_data_df['config_hash'] = training_data_hash\n",
    "combined_df['config_hash'] = combined_data_hash\n",
    "\n",
    "# Merge the x, y coordinates from training_data_df to combined_df based on the hash\n",
    "combined_df = combined_df.merge(training_data_df[['config_hash', 'x', 'y']], on='config_hash', how='left')\n",
    "\n",
    "# Cleanup if necessary (drop the hash column if no longer needed)\n",
    "combined_df.drop(columns=['config_hash'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop x_x and y_x\n",
    "combined_df.drop(columns=['x_x', 'y_x'], inplace=True)\n",
    "# rename x_y to x, and y_y to y\n",
    "combined_df.rename(columns={'x_y': 'x', 'y_y': 'y'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the dummy columns with the original non-one-hot encoded columns\n",
    "# First, drop the one-hot encoded columns\n",
    "combined_df.drop(columns=chessboard_dummy_columns, inplace=True)\n",
    "\n",
    "# Then, concatenate the original chessboard columns\n",
    "combined_df[chessboard_columns] = df[chessboard_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store csv as all_openings_projected_<timestamp>.csv\n",
    "combined_df.to_csv('all_openings_projected_' + now.strftime(\"%Y-%m-%d_%H-%M-%S\") + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename opening_type to algo, Result to cp, game_number to line\n",
    "combined_df.rename(columns={'opening_type': 'algo', 'Result': 'cp', 'game_number': 'line'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store csv as all_openings_projected_PSE_format_<timestamp>.csv\n",
    "combined_df.to_csv('all_openings_projected_PSE_format' + now.strftime(\"%Y-%m-%d_%H-%M-%S\") + '.csv', index=False)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
