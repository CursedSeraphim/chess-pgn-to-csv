{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 8018\n",
    "drop_duplicates_for_projection = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Work\\ETH Zurich\\IVIA\\chess-pgn-to-csv\\.venv310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Work\\ETH Zurich\\IVIA\\chess-pgn-to-csv\\.venv310\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 4, 4, 16)          1888      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 2, 2, 32)          4640      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 1, 1, 64)          18496     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 1, 1, 128)         73856     \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 1, 1, 256)         295168    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               25700     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 202       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 440150 (1.68 MB)\n",
      "Trainable params: 440150 (1.68 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define the network\n",
    "import tensorflow as tf\n",
    "dims = (8, 8, 13)\n",
    "n_components = 2\n",
    "encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=dims),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=16, kernel_size=3, strides=(2, 2), activation=\"relu\", padding=\"same\"\n",
    "    ),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=32, kernel_size=3, strides=(2, 2), activation=\"relu\", padding=\"same\"\n",
    "    ),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=64, kernel_size=3, strides=(2, 2), activation=\"relu\", padding=\"same\"\n",
    "    ),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=128, kernel_size=3, strides=(2, 2), activation=\"relu\", padding=\"same\"\n",
    "    ),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=256, kernel_size=3, strides=(2, 2), activation=\"relu\", padding=\"same\"\n",
    "    ),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=100),\n",
    "    tf.keras.layers.Dense(units=100),\n",
    "    tf.keras.layers.Dense(units=100),\n",
    "    tf.keras.layers.Dense(units=n_components),\n",
    "])\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_fit_kwargs = {\"callbacks\": [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='loss',\n",
    "        min_delta=10**-2,\n",
    "        patience=10,\n",
    "        verbose=1,\n",
    "    )\n",
    "]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('all_openings.csv')\n",
    "\n",
    "# One-hot encode the chessboard columns\n",
    "expected_categories = ['wp', 'wr', 'wn', 'wb', 'wq', 'wk', 'bp', 'br', 'bn', 'bb', 'bq', 'bk', '']\n",
    "\n",
    "# Define chessboard columns\n",
    "chessboard_columns = [f\"{col}{row}\" for row in range(1, 9) for col in \"abcdefgh\"]\n",
    "\n",
    "for column in chessboard_columns:\n",
    "    df[column] = pd.Categorical(df[column], categories=expected_categories)\n",
    "\n",
    "\n",
    "training_data_df = df[chessboard_columns]\n",
    "\n",
    "training_data_df = pd.get_dummies(training_data_df, columns=chessboard_columns)\n",
    "\n",
    "# Combine the one-hot encoded chessboard DataFrame with the rest of the metadata\n",
    "# First, drop the original chessboard columns from the main DataFrame to avoid duplicates\n",
    "metadata_df = df.drop(columns=chessboard_columns)\n",
    "# Then, concatenate the encoded chessboard DataFrame with the metadata DataFrame\n",
    "combined_df = pd.concat([metadata_df, training_data_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before dropping duplicates: 36155\n",
      "Length after dropping duplicates: 7079\n"
     ]
    }
   ],
   "source": [
    "if drop_duplicates_for_projection\n",
    "    # drop duplicates for projection, use the column names from chessboard_dummies_df for finding duplicates\n",
    "    print(\"Length before dropping duplicates:\", len(training_data_df))\n",
    "    training_data_df.drop_duplicates(subset=training_data_df.columns, inplace=True)\n",
    "    print(\"Length after dropping duplicates:\", len(training_data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (7079, 832)\n",
      "222/222 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the data correctly for the encoder\n",
    "# get numpy data out of training_data_df\n",
    "train_data = training_data_df.to_numpy()\n",
    "\n",
    "# Verify the shape\n",
    "print(\"Train shape:\", train_data.shape)\n",
    "\n",
    "# Generate embeddings using the encoder just to verify the shape\n",
    "train_embeddings = encoder.predict(train_data.reshape(-1, 8, 8, 13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap.parametric_umap import ParametricUMAP\n",
    "\n",
    "reducer = ParametricUMAP(\n",
    "    verbose=True,\n",
    "    keras_fit_kwargs = keras_fit_kwargs,\n",
    "    encoder=encoder,\n",
    "    dims=dims,\n",
    "    random_state=seed,\n",
    "    n_training_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before fitting, check dims: (8, 8, 13)\n",
      "Reducer expected input shape: (8, 8, 13)\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Work\\ETH Zurich\\IVIA\\chess-pgn-to-csv\\.venv310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "2034/2034 [==============================] - 131s 63ms/step - loss: 0.2330\n",
      "Epoch 2/10\n",
      "2034/2034 [==============================] - 133s 66ms/step - loss: 0.2048\n",
      "Epoch 3/10\n",
      "2034/2034 [==============================] - 129s 63ms/step - loss: 0.2020\n",
      "Epoch 4/10\n",
      "2034/2034 [==============================] - 137s 67ms/step - loss: 0.2009\n",
      "Epoch 5/10\n",
      "2034/2034 [==============================] - 135s 66ms/step - loss: 0.2003\n",
      "Epoch 6/10\n",
      "2034/2034 [==============================] - 129s 64ms/step - loss: 0.1999\n",
      "Epoch 7/10\n",
      "2034/2034 [==============================] - 131s 65ms/step - loss: 0.1996\n",
      "Epoch 8/10\n",
      "2034/2034 [==============================] - 130s 64ms/step - loss: 0.1994\n",
      "Epoch 9/10\n",
      "2034/2034 [==============================] - 135s 67ms/step - loss: 0.1990\n",
      "Epoch 10/10\n",
      "2034/2034 [==============================] - 130s 64ms/step - loss: 0.1988\n",
      "Embedding shape after fit_transform: (7079, 2)\n"
     ]
    }
   ],
   "source": [
    "import colorama\n",
    "reducer = ParametricUMAP(encoder=encoder, dims=dims)\n",
    "print(\"Before fitting, check dims:\", dims)\n",
    "print(\"Reducer expected input shape:\", reducer.dims)\n",
    "# Attempt to fit and transform\n",
    "try:\n",
    "    embedding = reducer.fit_transform(train_data.reshape((train_data.shape[0], -1)))\n",
    "    print(f\"Embedding shape after fit_transform: {embedding.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"{colorama.Fore.RED}Error during fit_transform: {e}{colorama.Style.RESET_ALL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from umap.parametric_umap import load_ParametricUMAP\n",
    "# embedder = load_ParametricUMAP('/your/path/here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/parametric_umap_embeddings_2024-03-21_16-02-03\\encoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/parametric_umap_embeddings_2024-03-21_16-02-03\\encoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras encoder model saved to ./embeddings/parametric_umap_embeddings_2024-03-21_16-02-03\\encoder\n",
      "INFO:tensorflow:Assets written to: ./embeddings/parametric_umap_embeddings_2024-03-21_16-02-03\\parametric_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/parametric_umap_embeddings_2024-03-21_16-02-03\\parametric_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras full model saved to ./embeddings/parametric_umap_embeddings_2024-03-21_16-02-03\\parametric_model\n",
      "Pickle of ParametricUMAP model saved to ./embeddings/parametric_umap_embeddings_2024-03-21_16-02-03\\model.pkl\n"
     ]
    }
   ],
   "source": [
    "# create unique filename based on date and time\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "reducer.save('./embeddings/parametric_umap_embeddings_' + now.strftime(\"%Y-%m-%d_%H-%M-%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# list of chess board dummy columns (to be used for hashing) = training_data_df columns\n",
    "chessboard_dummy_columns = training_data_df.columns\n",
    "\n",
    "# Add Projection Coordinates to Training Data DataFrame\n",
    "training_data_df[\"x\"] = embedding[:, 0]\n",
    "training_data_df[\"y\"] = embedding[:, 1]\n",
    "\n",
    "# Generate a unique hash for each row's configuration in both DataFrames\n",
    "def generate_row_hash(df, prefix_columns):\n",
    "    return df[prefix_columns].apply(lambda x: hash(tuple(x)), axis=1)\n",
    "\n",
    "# Generate hashes for both DataFrames\n",
    "training_data_hash = generate_row_hash(training_data_df, chessboard_dummy_columns)\n",
    "combined_data_hash = generate_row_hash(combined_df, chessboard_dummy_columns)  # Only the one-hot encoded columns\n",
    "\n",
    "# Add these hashes as a column to both DataFrames\n",
    "training_data_df['config_hash'] = training_data_hash\n",
    "combined_df['config_hash'] = combined_data_hash\n",
    "\n",
    "# Merge the x, y coordinates from training_data_df to combined_df based on the hash\n",
    "combined_df = combined_df.merge(training_data_df[['config_hash', 'x', 'y']], on='config_hash', how='left')\n",
    "\n",
    "# Cleanup if necessary (drop the hash column if no longer needed)\n",
    "combined_df.drop(columns=['config_hash'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last_move</th>\n",
       "      <th>from_square</th>\n",
       "      <th>to_square</th>\n",
       "      <th>Event</th>\n",
       "      <th>Site</th>\n",
       "      <th>Date</th>\n",
       "      <th>Round</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Result</th>\n",
       "      <th>...</th>\n",
       "      <th>h8_wk</th>\n",
       "      <th>h8_bp</th>\n",
       "      <th>h8_br</th>\n",
       "      <th>h8_bn</th>\n",
       "      <th>h8_bb</th>\n",
       "      <th>h8_bq</th>\n",
       "      <th>h8_bk</th>\n",
       "      <th>h8_</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>????.??.??</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>*</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.650878</td>\n",
       "      <td>0.503384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nh3</td>\n",
       "      <td>g1</td>\n",
       "      <td>h3</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>????.??.??</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>*</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.699159</td>\n",
       "      <td>0.640763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>????.??.??</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>*</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.650878</td>\n",
       "      <td>0.503384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nh3</td>\n",
       "      <td>g1</td>\n",
       "      <td>h3</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>????.??.??</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>*</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.699159</td>\n",
       "      <td>0.640763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d5</td>\n",
       "      <td>d7</td>\n",
       "      <td>d5</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>????.??.??</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>*</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.717516</td>\n",
       "      <td>0.843047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 849 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  last_move from_square to_square Event Site        Date Round White Black  \\\n",
       "0      None        None      None     ?    ?  ????.??.??     ?     ?     ?   \n",
       "1       Nh3          g1        h3     ?    ?  ????.??.??     ?     ?     ?   \n",
       "2      None        None      None     ?    ?  ????.??.??     ?     ?     ?   \n",
       "3       Nh3          g1        h3     ?    ?  ????.??.??     ?     ?     ?   \n",
       "4        d5          d7        d5     ?    ?  ????.??.??     ?     ?     ?   \n",
       "\n",
       "  Result  ... h8_wk h8_bp h8_br  h8_bn  h8_bb  h8_bq  h8_bk  h8_         x  \\\n",
       "0      *  ...     0     0     1      0      0      0      0    0  1.650878   \n",
       "1      *  ...     0     0     1      0      0      0      0    0  1.699159   \n",
       "2      *  ...     0     0     1      0      0      0      0    0  1.650878   \n",
       "3      *  ...     0     0     1      0      0      0      0    0  1.699159   \n",
       "4      *  ...     0     0     1      0      0      0      0    0  1.717516   \n",
       "\n",
       "          y  \n",
       "0  0.503384  \n",
       "1  0.640763  \n",
       "2  0.503384  \n",
       "3  0.640763  \n",
       "4  0.843047  \n",
       "\n",
       "[5 rows x 849 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop x_x and y_x\n",
    "combined_df.drop(columns=['x_x', 'y_x'], inplace=True)\n",
    "# rename x_y to x, and y_y to y\n",
    "combined_df.rename(columns={'x_y': 'x', 'y_y': 'y'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the dummy columns with the original non-one-hot encoded columns\n",
    "# First, drop the one-hot encoded columns\n",
    "combined_df.drop(columns=chessboard_dummy_columns, inplace=True)\n",
    "\n",
    "# Then, concatenate the original chessboard columns\n",
    "combined_df[chessboard_columns] = df[chessboard_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store csv as all_openings_projected.csv\n",
    "combined_df.to_csv('all_openings_projected.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename opening_type to algo, Result to cp, game_number to line\n",
    "combined_df.rename(columns={'opening_type': 'algo', 'Result': 'cp', 'game_number': 'line'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store csv as all_openings_projected.csv\n",
    "combined_df.to_csv('all_openings_projected_PSE_format.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
